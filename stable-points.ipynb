{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers import logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "from torchvision import transforms as tfms\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6cce99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8769a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
    "    return 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "\n",
    "def create_movie(dir,movie_name,fps=12):\n",
    "    !ffmpeg -v 1 -y -f image2 -framerate {fps} -i {dir}/%04d.jpg -c:v libx264 -preset slow -qp 18 -pix_fmt yuv420p {movie_name}\n",
    "\n",
    "def embed_movie(movie_name):\n",
    "    mp4 = open(movie_name,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return\"\"\"\n",
    "    <video width=600 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da574df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_embedding(prompt, encoded, start_step=10, seed=1):\n",
    "    # Settings (same as before except for the new prompt)\n",
    "    prompt = [prompt]\n",
    "    height = 512                        # default height of Stable Diffusion\n",
    "    width = 512                         # default width of Stable Diffusion\n",
    "    num_inference_steps = 50            # Number of denoising steps\n",
    "    guidance_scale = 7.5                  # Scale for classifier-free guidance\n",
    "    generator = torch.manual_seed(seed)   # Seed generator to create the inital latent noise\n",
    "    batch_size = 1\n",
    "\n",
    "    # Prep text (same as before)\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    # Prep Scheduler (setting the number of inference steps)\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    if encoded is None:\n",
    "        # Prep latents\n",
    "        start_step = -1\n",
    "        latents = torch.randn(\n",
    "        (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "        )\n",
    "        latents = latents.to(torch_device)\n",
    "        latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n",
    "    else:\n",
    "        # Prep latents (noising appropriately for start_step)\n",
    "        #start_step = 10\n",
    "        start_sigma = scheduler.sigmas[start_step]\n",
    "        noise = torch.randn_like(encoded)\n",
    "        latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
    "        latents = latents.to(torch_device).float()\n",
    "\n",
    "    # Loop\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "        if i > start_step-1: # << This is the only modification to the loop we do\n",
    "            \n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            sigma = scheduler.sigmas[i]\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    return latents_to_pil(latents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_diffusion(prompt,encoded,outdir,starting_iteration=1, iterations=10, start_step=10,seed=1, increment_seed=True):\n",
    "    print(f\"Number of iterations: {iterations}\")\n",
    "    im_latents = encoded\n",
    "    imgs = []\n",
    "    \n",
    "    for f in tqdm(range(starting_iteration, starting_iteration + iterations)):\n",
    "        im = generate_image_from_embedding(prompt,im_latents, start_step, seed)\n",
    "        if increment_seed: seed = seed + 1\n",
    "        im_latents = pil_to_latent(im)\n",
    "        imgs.append(im)\n",
    "        im.save(f'{outdir}/{f:04}.jpg')\n",
    "        print(f\"Saved {outdir}/{f:04}.jpg\")\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c566418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_prompt, prompt = \"london bus\", \"cat\"\n",
    "#initial_prompt, prompt = \"london bus\", \"dog\"\n",
    "initial_prompt, prompt = \"mount fuji in spring\", \"cat\"\n",
    "#initial_prompt, prompt = \"mount fuji in spring\", \"an astronaut on a horse, photo\"\n",
    "initial_prompt, prompt = \"mount fuji in spring\", \"car\"\n",
    "\n",
    "increment_seed = True\n",
    "initial_image_seed, diffusion_seed = 1000, 1001\n",
    "start_step = 10\n",
    "iterations =  200\n",
    "starting_iteration = 1\n",
    "exp_label = f\"{initial_prompt}-{prompt}-startstep-{start_step}-incrementseed-{increment_seed}-seeds-{initial_image_seed}-{diffusion_seed}\"\n",
    "img_dir = f'frames-{exp_label}'.replace(' ','_')\n",
    "movie = f'movie-{exp_label}.mp4'.replace(' ','_')\n",
    "\n",
    "def get_existing_experiment_or_create_new(img_dir):\n",
    "    imgs = []\n",
    "    \n",
    "    if not os.path.exists(img_dir): \n",
    "        os.mkdir(img_dir)\n",
    "        img = generate_image_from_embedding(initial_prompt,None,seed=initial_image_seed)\n",
    "        imgs.append(img)\n",
    "        img.save(f'{img_dir}/{0:04}.jpg')\n",
    "    else:\n",
    "        print(\"Image directory already exists. Continuing image generation...\")\n",
    "        # find highest numbered image\n",
    "        img_files = glob.glob(f'{img_dir}/*.jpg')\n",
    "        img_files.sort()\n",
    "        imgs = [Image.open(img_file) for img_file in img_files]\n",
    "        print(f\"Lowest numbered image (original image): {img_files[0]}\")\n",
    "        print(f\"Highest numbered image (starting with this image): {img_files[-1]}\")\n",
    "        img = Image.open(img_files[-1])\n",
    "\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "imgs = get_existing_experiment_or_create_new(img_dir)\n",
    "generated_latents = [pil_to_latent(img) for img in imgs]\n",
    "starting_iteration = len(imgs)\n",
    "diffusion_seed = diffusion_seed + len(imgs)\n",
    "img = imgs[-1]\n",
    "latent = pil_to_latent(img)\n",
    "\n",
    "# plot initial image\n",
    "plt.imshow(img)\n",
    "\n",
    "# run diffusion and append to list of images\n",
    "imgs_new = loop_diffusion(prompt,latent,img_dir,starting_iteration=starting_iteration, iterations=iterations, start_step=start_step,seed=1,increment_seed=increment_seed)\n",
    "imgs.extend(imgs_new)\n",
    "generated_latents = [pil_to_latent(img) for img in imgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_movie(img_dir,movie,fps=6)\n",
    "HTML(embed_movie(movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ncols = 10\n",
    "nrows = int(np.ceil(len(imgs)/ncols))\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if i*ncols + j < len(imgs):\n",
    "            axs[i,j].imshow(imgs[i*ncols+j])\n",
    "            axs[i,j].axis('off')\n",
    "            #axs[i,j].set_title(f\"{i*ncols+j:04}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do dimensionality reduction on the latents\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plot_latents = [l.cpu().numpy().flatten() for l in generated_latents]\n",
    "print(f\"Number of latents: {len(plot_latents)}\")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(np.vstack(plot_latents))\n",
    "pca_latents = pca.transform(np.vstack(plot_latents))\n",
    "\n",
    "# plot the latents with a number next to each point representing the order of the latent\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(pca_latents[:,0],pca_latents[:,1])\n",
    "for i, (x,y) in enumerate(pca_latents):\n",
    "    ax.text(x,y,i,fontsize=10)\n",
    "\n",
    "fig.savefig(f'latent-space-{exp_label}.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b97ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you hover over a point, it will show the image\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.cbook import get_sample_data\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(pca_latents[:,0],pca_latents[:,1])\n",
    "for i, (x,y) in enumerate(pca_latents):\n",
    "    if i % 5 == 0:\n",
    "        ab = AnnotationBbox(OffsetImage(imgs[i], zoom=0.1), (x, y), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "        ax.text(x,y,i,fontsize=10)\n",
    "\n",
    "#ax.axis('off')\n",
    "# save plot \n",
    "fig.savefig(f'latent-space-with-images-{exp_label}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ae10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_label_compare = 'frames-london_bus-cat-startstep-10-incrementseed-True-seeds-1-1'\n",
    "#exp_label_compare = 'frames-london_bus-cat-startstep-10-incrementseed-True-seeds-100-101'\n",
    "imgs_compare = get_existing_experiment_or_create_new(exp_label_compare)\n",
    "generated_latents_compare = [pil_to_latent(img) for img in imgs_compare]\n",
    "plot_latents_compare = [l.cpu().numpy().flatten() for l in generated_latents_compare]\n",
    "\n",
    "# calculate the distance between each latent in plot_latents and the latent in plot_latents_compare with the same index\n",
    "distances = []\n",
    "for i in range(len(plot_latents_compare)):\n",
    "    distances.append(np.linalg.norm(plot_latents[i] - plot_latents_compare[i]))\n",
    "\n",
    "# plot the distances\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.plot(distances)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Distance')\n",
    "fig.savefig(f'distance-{exp_label_compare}-{exp_label}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_label_compare = 'frames-london_bus-cat-startstep-10-incrementseed-True-seeds-1-1'\n",
    "#exp_label_compare = 'frames-london_bus-cat-startstep-10-incrementseed-True-seeds-100-101'\n",
    "imgs_compare = get_existing_experiment_or_create_new(exp_label_compare)\n",
    "print(f\"len(img): {len(imgs)} len(imgs_compare): {len(imgs_compare)}\")\n",
    "generated_latents_compare = [pil_to_latent(img) for img in imgs_compare]\n",
    "plot_latents_compare = [l.cpu().numpy().flatten() for l in generated_latents_compare]\n",
    "\n",
    "# do dimensionality reduction on the generated_latents and generated_latents_compare\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "#pca.fit(np.vstack(np.concatenate([plot_latents, plot_latents_compare])))\n",
    "pca.fit(np.vstack(plot_latents))\n",
    "pca_latents = pca.transform(np.vstack(plot_latents))\n",
    "pca_latents_compare = pca.transform(np.vstack(plot_latents_compare))\n",
    "\n",
    "print(f\"len of pca_latents: {len(pca_latents)} len of pca_latents_compare: {len(pca_latents_compare)}\")\n",
    "\n",
    "# plot the latents with a number next to each point representing the order of the latent, with different colors for the two sets of latents\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(pca_latents[:,0],pca_latents[:,1],color='blue')\n",
    "ax.scatter(pca_latents_compare[:,0],pca_latents_compare[:,1],color='red')\n",
    "for i, (x,y) in enumerate(pca_latents):\n",
    "    ax.text(x,y,i,fontsize=10)\n",
    "for i, (x,y) in enumerate(pca_latents_compare):\n",
    "    ax.text(x,y,i,fontsize=10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea4f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "428.993px",
    "width": "279.983px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
